
<!DOCTYPE html>
<html>

<head lang="en">
    <!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-48YJ5MFMGM"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-48YJ5MFMGM');
</script>
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>MGG</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- <base href="/"> -->

        <!--FACEBOOK-->
    <meta property="og:image" content="https://jonbarron.info/mipnerf/img/rays_square.png">
    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="682">
    <meta property="og:image:height" content="682">
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://reyllama.github.io/OneR"/>
    <meta property="og:title" content="Fashion-Diffusion" />
    <meta property="og:description" content="Project page for Unifying Vision-Language Representation Space with Single-tower Transformer" />




<!--     <link rel="apple-touch-icon" href="apple-touch-icon.png"> -->
  <!-- <link rel="icon" type="image/png" href="img/seal_icon.png"> -->
    <!-- Place favicon.ico in the root directory -->

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <link rel="stylesheet" href="css/bootstrap.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>

    <script src="js/app.js"></script>
</head>

<body>
    <div class="container" id="main">
        <div class="row">
            <h2 class="col-md-12 text-center">
                Fashion Style Editing with</br> Generative Human Prior<br>
                <!-- <small>
                    ICLR 2023 Workshop on Multimodal Representation Learning (Spotlight)
                </small> -->
            </h2>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        <a href="https://reyllama.github.io">
                          Chaerin Kong*
                        </a>
                        </br>NXN Labs
                    </li>
                    <li>
                        <a href="https://www.linkedin.com/in/%EC%9D%B4%EC%8A%B9%EC%9A%A9/">
                          Seungyong Lee*
                        </a>
                        </br>NXN Labs
                    </li>
                    <li>
                        <a href="https://www.linkedin.com/in/soohyeok-im-638701155/">
                          Soohyeok Im*
                        </a>
                        </br>NXN Labs
                    </li>
                    <li>
                        <a href="https://www.linkedin.com/in/wonsuk-yang-b49929223/">
                          Wonsuk Yang*
                        </a>
                        </br>NXN Labs
                    </li>
                </ul>
            </div>
        </div>


        <div class="row">
                <div class="col-md-4 col-md-offset-4 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <a href="https://reyllama.github.io/fase/img/FaSE_NXNLABS.pdf">
                            <image src="img/mip_paper_image.jpg" height="60px">
                                <h4><strong>Paper</strong></h4>
                            </a>
                        </li>
                        <li>
                            <!-- <a href="https://github.com/reyllama/caption-diffusion"> -->
                            <image src="img/github.png" height="60px">
                                <h4><strong>Code (coming soon)</strong></h4>
                            <!-- </a> -->
                        </li>
                    </ul>
                </div>
        </div>



        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <image src="img/fig1.jpg" class="img-responsive" alt="overview"></image>
                <h3>
                    Abstract
                </h3>
<!--                <image src="img/pipeline.jpeg" class="img-responsive" alt="overview"><br>-->
                <p class="text-justify">
                    Image editing has been a long-standing challenge in the research community with its far-reaching impact on numerous applications. Recently, text-driven methods started to deliver promising results in domains like human faces, but their applications to more complex domains have been relatively limited. In this work, we explore the task of fashion style editing, where we aim to manipulate the fashion style of human imagery using text descriptions. Specifically, we leverage a generative human prior and achieve fashion style editing by navigating its learned latent space. We first verify that the existing text-driven editing methods fall short for our problem due to their overly simplified guidance signal, and propose two directions to reinforce the guidance: textual augmentation and visual referencing. Combined with our empirical findings on the latent space structure, our Fashion Style Editing framework (FaSE) successfully projects abstract fashion concepts onto human images and introduces exciting new applications to the field.
                </p>
            </div>
        </div>



<!--        <div class="row">-->
<!--            <div class="col-md-8 col-md-offset-2">-->
<!--                <h3>-->
<!--                    Video-->
<!--                </h3>-->
<!--                <div class="text-center">-->
<!--                    <div style="position:relative;padding-top:56.25%;">-->
<!--                        <iframe src="https://www.youtube.com/embed/EpH175PY1A0" allowfullscreen style="position:absolute;top:0;left:0;width:100%;height:100%;"></iframe>-->
<!--                    </div>-->
<!--                </div>-->
<!--            </div>-->
<!--        </div>-->


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Problem: Fashion Style Editing
                </h3>
                <p>
                    Our goal is to develop a learning-based system that can manipulate the fashion style of human imagery using text descriptions.
                    This task has a wide range of relevant applications, including virtual try-on, product design and content creation.
                    Despite being a subtask of general image editing, it poses a unique set of challenges due to the innate elusiveness of <em>fashion style concepts (e.g., 'street')</em> and the need for delicate modification of whole-body human images.
                </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Approach: FaSE
                </h3>
                <image src="img/fig3.jpeg" class="img-responsive" alt="overview">
                <br>
                <p>
                    We propose Fashion Style Editing (FaSE), a novel framework that leverages a generative human prior to achieve fashion style editing by navigating its learned latent space.
                    We choose StyleGAN-Human (<em>Fu and Li et al. (2022)</em>) as our generative prior, for its impressive capacity for whole human image generation.
                </p>
                <br>
                <p>
                    Upon this pretrained human generation system, we envision something like StyleCLIP (<em>Patashnik et al. (2021)</em>).
                    Hence, we initially run experiments with naive StyleCLIP latent-mapper implementation with little success as illustrated below.
                </p>
                <br>
                <image src="img/fig2_1.png" class="img-responsive" alt="overview"></image>
                <br>
                <p>
                    We find that the existing text-driven editing methods fall short for our problem due to their overly simplified guidance signal.
                    To address this, we propose two directions to reinforce the guidance: textual augmentation and visual referencing.
                    The former involves generating additional text descriptions to enrich the guidance signal, while the latter involves providing visual references to the model by retrieving relevant images from our fashion database.
                    These two strategies successfully enhance the guidance signal and enable fashion style editing, as shown below.
                </p>
                <br>
                <image src="img/fig2_2.png" class="img-responsive" alt="overview"></image>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Experiments
                </h3>
                <image src="img/fig5.jpg" class="img-responsive" alt="overview"></image>
                <br>
                <p>
                    Evident from the above figure, our FaSE framework successfully projects abstract fashion concepts with minimal irrelevant modification to the base image.
                    We can see that applying naive StyleCLIP method to fashion domain fails in visualizing high-level target concepts.
                </p>
                <br>
                <image src="img/fig4.jpg" class="img-responsive" alt="overview"></image>
                <br>
                <p>
                    We further share insights on the latent space structure of StyleGAN-Human. 
                    When we compartmentalize the <em>W+</em> space into three sections as done in StyleCLIP, we empirically discover that 
                    mid and fine layers mostly contribute to fashion style editing, while the coarse layer is mainly responsible for the posture of the generated human.
                    Among these two, the mid layer is more sensitive to the high-level fashion concepts (<em>e.g., 'formal'</em>), while the fine layer is more sensitive to texture details (<em>e.g., 'blue color'</em>).
                </p>
                <br>
            </div>
        </div>





    </div>
</body>
</html>
