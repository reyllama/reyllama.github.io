
<!DOCTYPE html>
<html>

<head lang="en">
    <!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-48YJ5MFMGM"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-48YJ5MFMGM');
</script>
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>MGG</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- <base href="/"> -->

        <!--FACEBOOK-->
    <meta property="og:image" content="https://jonbarron.info/mipnerf/img/rays_square.png">
    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="682">
    <meta property="og:image:height" content="682">
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://reyllama.github.io/OneR"/>
    <meta property="og:title" content="Fashion-Diffusion" />
    <meta property="og:description" content="Project page for Unifying Vision-Language Representation Space with Single-tower Transformer" />




<!--     <link rel="apple-touch-icon" href="apple-touch-icon.png"> -->
  <!-- <link rel="icon" type="image/png" href="img/seal_icon.png"> -->
    <!-- Place favicon.ico in the root directory -->

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <link rel="stylesheet" href="css/bootstrap.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>

    <script src="js/app.js"></script>
</head>

<body>
    <div class="container" id="main">
        <div class="row">
            <h2 class="col-md-12 text-center">
                Fashion Style Editing with</br> Generative Human Prior<br>
                <!-- <small>
                    ICLR 2023 Workshop on Multimodal Representation Learning (Spotlight)
                </small> -->
            </h2>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        <a href="https://reyllama.github.io">
                          Chaerin Kong*
                        </a>
                        </br>NXN Labs
                    </li>
                    <li>
                        <a href="https://www.linkedin.com/in/%EC%9D%B4%EC%8A%B9%EC%9A%A9/">
                          Seungyong Lee*
                        </a>
                        </br>NXN Labs
                    </li>
                    <li>
                        <a href="https://www.linkedin.com/in/soohyeok-im-638701155/">
                          Soohyeok Im*
                        </a>
                        </br>NXN Labs
                    </li>
                    <li>
                        <a href="https://www.linkedin.com/in/wonsuk-yang-b49929223/">
                          Wonsuk Yang*
                        </a>
                        </br>NXN Labs
                    </li>
                </ul>
            </div>
        </div>


        <div class="row">
                <div class="col-md-4 col-md-offset-4 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <a href="https://github.com/reyllama/reyllama.github.io/fase/img/FaSE_NXNLABS.pdf">
                            <image src="img/mip_paper_image.jpg" height="60px">
                                <h4><strong>Paper</strong></h4>
                            </a>
                        </li>
                        <li>
                            <!-- <a href="https://github.com/reyllama/caption-diffusion"> -->
                            <image src="img/github.png" height="60px">
                                <h4><strong>Code (coming soon)</strong></h4>
                            <!-- </a> -->
                        </li>
                    </ul>
                </div>
        </div>



        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <image src="img/fig1.jpg" class="img-responsive" alt="overview"></image>
                <h3>
                    Abstract
                </h3>
<!--                <image src="img/pipeline.jpeg" class="img-responsive" alt="overview"><br>-->
                <p class="text-justify">
                    Image editing has been a long-standing challenge in the research community with its far-reaching impact on numerous applications. Recently, text-driven methods started to deliver promising results in domains like human faces, but their applications to more complex domains have been relatively limited. In this work, we explore the task of fashion style editing, where we aim to manipulate the fashion style of human imagery using text descriptions. Specifically, we leverage a generative human prior and achieve fashion style editing by navigating its learned latent space. We first verify that the existing text-driven editing methods fall short for our problem due to their overly simplified guidance signal, and propose two directions to reinforce the guidance: textual augmentation and visual referencing. Combined with our empirical findings on the latent space structure, our Fashion Style Editing framework (FaSE) successfully projects abstract fashion concepts onto human images and introduces exciting new applications to the field.
                </p>
            </div>
        </div>



<!--        <div class="row">-->
<!--            <div class="col-md-8 col-md-offset-2">-->
<!--                <h3>-->
<!--                    Video-->
<!--                </h3>-->
<!--                <div class="text-center">-->
<!--                    <div style="position:relative;padding-top:56.25%;">-->
<!--                        <iframe src="https://www.youtube.com/embed/EpH175PY1A0" allowfullscreen style="position:absolute;top:0;left:0;width:100%;height:100%;"></iframe>-->
<!--                    </div>-->
<!--                </div>-->
<!--            </div>-->
<!--        </div>-->


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Problem: Fashion Style Editing
                </h3>
                <p>
                    Our goal is to develop a learning-based system that can manipulate the fashion style of human imagery using text descriptions.
                    This task has a wide range of relevant applications, including virtual try-on, product design and content creation.
                    Despite being a subtask of general image editing, it poses a unique set of challenges due to the innate elusiveness of <em>fashion style concepts (e.g., 'street')</em> and the need for delicate modification of whole-body human images.
                </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Approach: FaSE
                </h3>
                <image src="img/fig3.jpeg" class="img-responsive" alt="overview">
                <br>
                <p>
                    We propose Fashion Style Editing (FaSE), a novel framework that leverages a generative human prior to achieve fashion style editing by navigating its learned latent space.
                    We choose StyleGAN-Human (<em>Fu and Li et al. (2022)</em>) as our generative prior, for its impressive capacity for whole human image generation.
                </p>
                <br>
                <image src="img/fig5.jpg" class="img-responsive" alt="overview">
                <br>
                <p>
                    <em>5. CAP is a more indirect if not challenging form of supervision than ITC or ITM.</em>
                </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    New Baseline: Guidance Shift
                </h3>
                <p>
                    Based on the above findings, we propose a simple yet effective modification to the previous regime that takes advantage of both ends.
                    Specifically, we start from <em>CAP</em> guidance to outline the overall scene structure and gradually shift towards <em>ITC</em> for refined details.
                    We compare this with naive baselines, simple <em>ITC</em>, simple <em>CAP</em> and <em>BLEND</em>, where we use these two signals but simply mix them without the gradual transition.
                    We report both qualitative and quantitative human evaluations. Ours not only supports our empirical insights but also improves complex scene generation in an extremely straightforward manner.
                </p>
                <image src="img/fig7.jpg" class="img-responsive" alt="overview">
                    <br>
                <image src="img/fig6.jpg" class="img-responsive" alt="overview">
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Additional Results
                </h3>
                <p>
                    We hereby display additional examples that further clarifies our analysis.
                </p>
                <image src="img/sup1.jpg" class="img-responsive" alt="overview">
                    <br>
                <image src="img/sup2.jpg" class="img-responsive" alt="overview">
                <br>
                <p>As opposed to <em>ITC</em> that generates realistic samples but the output severely oscillate even with minor typos, dense supervisions shows much better robustness, which can come in handy in a typical V-L setting
                where we rely on a massive web-crawled noisy image-text database.</p>
                <br>
                <image src="img/sup4.jpg" class="img-responsive" alt="overview">
                    <br>
                <image src="img/sup3.jpg" class="img-responsive" alt="overview">
                    <br><br>
            </div>
        </div>


<!--        <div class="row">-->
<!--            <div class="col-md-8 col-md-offset-2">-->
<!--                <h3>-->
<!--                    Citation-->
<!--                </h3>-->
<!--                <div class="form-group col-md-10 col-md-offset-1">-->
<!--                    <textarea id="bibtex" class="form-control" readonly>-->
<!--@article{barron2021mipnerf,-->
<!--    title={Mip-NeRF: A Multiscale Representation -->
<!--           for Anti-Aliasing Neural Radiance Fields},-->
<!--    author={Jonathan T. Barron and Ben Mildenhall and -->
<!--            Matthew Tancik and Peter Hedman and -->
<!--            Ricardo Martin-Brualla and Pratul P. Srinivasan},-->
<!--    journal={ICCV},-->
<!--    year={2021}-->
<!--}</textarea>-->
<!--                </div>-->
<!--            </div>-->
<!--        </div>-->

<!--        <div class="row">-->
<!--            <div class="col-md-8 col-md-offset-2">-->
<!--                <h3>-->
<!--                    Acknowledgements-->
<!--                </h3>-->
<!--                <p class="text-justify">-->
<!--                We thank Janne Kontkanen and David Salesin for their comments on the text, Paul Debevec for constructive discussions, and Boyang Deng for JaxNeRF. -->
<!--                    <br>-->
<!--                MT is funded by an NSF Graduate Fellowship.-->
<!--                    <br>-->
<!--                The website template was borrowed from <a href="http://mgharbi.com/">Michaël Gharbi</a>.-->
<!--                </p>-->
<!--            </div>-->
<!--        </div>-->
    </div>
</body>
</html>
