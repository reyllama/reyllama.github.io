<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Chaerin Kong</title>
  
  <meta name="author" content="Chaerin Kong">
  <meta name="viewport" content="width=device-width, initial-scale=1">

<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-48YJ5MFMGM"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-48YJ5MFMGM');
</script>


  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Chaerin Kong</name>
              </p>
              <p>I am a problem-driven AI researcher, lifting AI as a means of real world problem solving. Currently I am interested in leveraging AI to understand the inner workings of our mind (brain). I am responsible for AI applications at <a href="https://asleepglobal.imweb.me/">Asleep</a>, a sleep-tech startup that aims to measure and improve our sleep. I was extremely privileged to work with talented colleauges at <a href="https://twelvelabs.io/">Twelve Labs</a> and <a href="https://navercorp.com/en">NAVER</a>.
              </p>
              <p style="text-align:center">
                <a href="mailto:rin4616@gmail.com">Email</a> &nbsp/&nbsp
                <a href="data/chaerin-cv.pdf">CV</a> &nbsp/&nbsp
<!--                <a href="data/JonBarron-bio.txt">Bio</a> &nbsp/&nbsp-->
                <a href="https://scholar.google.com/citations?user=TownIFQAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/reyllama/">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/self2.jpeg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/self2_circle.jpeg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Published</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr onmouseout="malle_stop()" onmouseover="malle_start()">
            <td style="padding:20px;width:25%;vertical-align:top">
              <div class="one">
                <img src='images/concat2.png' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:top">
              <a href="https://reyllama.github.io/concat">
                <papertitle>ConcatPlexer: Additional Dim1 Batching for Faster ViTs</papertitle>
              </a>
              <br>
              <a href="https://www.linkedin.com/in/dhk1349/">Donghoon Han</a>,
              <a href="https://www.linkedin.com/in/shawn615/">Seunghyeun Seo</a>,
              <a href="https://www.linkedin.com/in/jdh3577/">Donghyeon Jeon</a>,
              <a href="https://www.linkedin.com/in/jiho-jang-58580b183/">Jiho Jang</a>,
              <strong>Chaerin Kong</strong>,
              <a href="http://mipal.snu.ac.kr/index.php/Nojun_Kwak">Nojun Kwak</a>
              <br>
              <em>NeurIPS</em> 2023 Workshop on Advancing Neural Network Training <em>(Oral)</em>
                 <br>
                 <a href="https://arxiv.org/abs/2308.11199">arXiv</a>
              <p></p>
              <p>
              We explore means to accelerate ViT inference by concatenating abstract visual tokens of multiple images along dim=1 and processing them at once.
              </p>
            </td>
          </tr>

          <tr onmouseout="malle_stop()" onmouseover="malle_start()">
            <td style="padding:20px;width:25%;vertical-align:top">
              <div class="one">
                <img src='AADiff/img/method14.png' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:top">
              <a href="https://reyllama.github.io/AADiff">
                <papertitle>AADiff: Audio-Aligned Video Synthesis with Text-to-Image Diffusion</papertitle>
              </a>
              <br>
              <a href="https://www.linkedin.com/in/seungwoo-dale-lee/">Seungwoo Lee</a>,
              <strong>Chaerin Kong</strong>,
              <a href="https://www.linkedin.com/in/jdh3577/">Donghyeon Jeon</a>,
              <a href="http://mipal.snu.ac.kr/index.php/Nojun_Kwak">Nojun Kwak</a>
              <br>
              <em>CVPR</em> 2023 Workshop on AI for Content Creation
                <br>
                <a href="https://arxiv.org/abs/2305.04001">arXiv</a>
                <p></p>
              <p>
              We introduce a simple framework for audio-aligned text-to-video synthesis that employs an off-the-shelf text-to-image diffusion model.
              </p>
            </td>
          </tr>

          <tr onmouseout="malle_stop()" onmouseover="malle_start()">
            <td style="padding:20px;width:25%;vertical-align:top">
              <div class="one">
                <img src='MGG/img/newfig1.jpg' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:top">
              <a href="https://reyllama.github.io/MGG">
                <papertitle>Analyzing Multimodal Objectives through the Lens of Generative Diffusion Guidance</papertitle>
              </a>
              <br>
              <strong>Chaerin Kong</strong>,
              <a href="http://mipal.snu.ac.kr/index.php/Nojun_Kwak">Nojun Kwak</a>
              <br>
              <em>ICLR</em> 2023 Workshop on Multimodal Representation Learning <em>(Spotlight)</em>
                <br>
                <a href="https://arxiv.org/abs/2302.10305">arXiv</a>
              /
              <a href="data/MGG_poster.png">poster</a>
                <p></p>
              <p>
              We study the semantic information encoded in widely used Vision-Language objectives (e.g., contrastive, captioning) by using each as diffusion guidance and inspecting the visualized images.
              </p>
            </td>
          </tr>

          <tr onmouseout="malle_stop()" onmouseover="malle_start()">
              <td style="padding:20px;width:25%;vertical-align:top">
                <div class="one">
                  <img src='images/OneR.png' width="160">
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:top">
                <a href="https://reyllama.github.io/OneR">
                  <papertitle>Unifying Vision-Language Representation Space with Single-tower Transformer</papertitle>
                </a>
                <br>
                <a href="https://www.linkedin.com/in/jiho-jang-58580b183/">Jiho Jang*</a>,
                <strong>Chaerin Kong*</strong>,
                <a href="https://scholar.google.com/citations?user=2kW3474AAAAJ&hl=ko">Donghyeon Jeon</a>,
                <a href="https://www.linkedin.com/in/seonhoon-kim-b9511410b/">Seonhoon Kim</a>,
                <a href="http://mipal.snu.ac.kr/index.php/Nojun_Kwak">Nojun Kwak</a>
                <br>
                <em>AAAI</em> 2023 <em>(Oral)</em>
                <br>
                <a href="https://arxiv.org/abs/2211.11153">arXiv</a>
                /
              <a href="data/OneR_poster.pdf">poster</a>
                <p></p>
                <p>
                We train a modality-agnostic Vision-Language model, OneR, and investigate intriguing properties of a unified V-L representation.
                </p>
              </td>
            </tr>

          <tr onmouseout="malle_stop()" onmouseover="malle_start()">
            <td style="padding:20px;width:25%;vertical-align:top">
              <div class="one">
                <img src='images/fashion-diffusion.png' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:top">
              <a href="https://reyllama.github.io/Fashion-Diffusion">
                <papertitle>Leveraging Off-the-shelf Diffusion Model for Multi-attribute Fashion Image Manipulation</papertitle>
              </a>
              <br>
              <strong>Chaerin Kong</strong>,
              <a href="https://scholar.google.com/citations?user=2kW3474AAAAJ&hl=ko">Donghyeon Jeon</a>,
              <a href="https://www.linkedin.com/in/ohjoon-kwon-11b309170/?originalSubdomain=kr">Ohjoon Kwon</a>,
              <a href="http://mipal.snu.ac.kr/index.php/Nojun_Kwak">Nojun Kwak</a>
              <br>
              <em>WACV</em> 2023
              <br>
              <a href="https://arxiv.org/abs/2210.05872">arXiv</a>
              /
              <a href="data/FD_poster.pdf">poster</a>
              <p></p>
              <p>
              We propose a mask-free fashion attribute editing framework that employs a pretrained diffuser and an efficiently finetuned guidance model.
              </p>
            </td>
          </tr>

        <tr onmouseout="malle_stop()" onmouseover="malle_start()">
            <td style="padding:20px;width:25%;vertical-align:top">
              <div class="one">
                <img src='images/SMIR.png' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:top">
              <papertitle>Self-distilled Self-supervised Representation Learning</papertitle>
              <br>
              <a href="https://www.linkedin.com/in/jiho-jang-58580b183/">Jiho Jang</a>,
              <a href="https://www.linkedin.com/in/seonhoon-kim-b9511410b/">Seonhoon Kim*</a>,
              <a href="http://mipal.snu.ac.kr/index.php/KiYoon_Yoo">KiYoon Yoo*</a>,
              <strong>Chaerin Kong</strong>,
              <a href="https://scholar.google.co.kr/citations?user=mzFEJVIAAAAJ&hl=ko">Jangho Kim</a>,
              <a href="http://mipal.snu.ac.kr/index.php/Nojun_Kwak">Nojun Kwak</a>
              <br>
                <em>WACV</em> 2023
                <p></p>
              <p>
                By encouraging self-distillation from lower layer to upper layer in traditional SSL frameworks (e.g., SimCLR, MoCo, BYOL), we can improve the representation quality as confirmed by various downstream task performances.
              </p>
            </td>
          </tr>

          <tr onmouseout="malle_stop()" onmouseover="malle_start()">
              <td style="padding:20px;width:25%;vertical-align:top">
                <div class="one">
                  <img src='images/CFNSG.jpg' width="160">
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:top">
                <papertitle>Towards Efficient Neural Scene Graphs by Learning Consistency Fields</papertitle>
                <br>
                <a href="https://scholar.google.co.kr/citations?user=2bp69mwAAAAJ&hl=ko">Yeji Song</a>,
                <strong>Chaerin Kong</strong>,
                <a href="https://www.linkedin.com/in/seoyoung-lee-8b4036177/">Seo Young Lee</a>,
                <a href="http://mipal.snu.ac.kr/index.php/Nojun_Kwak">Nojun Kwak</a>,
                <a href="http://www.joonseok.net/home.html">Joonseok Lee</a>
                <br>
                <em>BMVC</em> 2022
                <br>
                <a href="https://github.com/ldynx/CF-NSG">code</a>
                /
                <a href="https://arxiv.org/abs/2210.04127">arXiv</a>
                <p></p>
                <p>
                  <a href="https://light.princeton.edu/publication/neural-scene-graphs/">Neural Scene Graphs</a> can be rendered more efficiently and in a more controllable manner by learning the consistency field of a given scene.
                </p>
              </td>
            </tr>

          <tr onmouseout="malle_stop()" onmouseover="malle_start()">
            <td style="padding:20px;width:25%;vertical-align:top">
              <div class="one">
                <img src='images/mixdl_overview.png' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:top">
              <a href="https://reyllama.github.io/MixDL">
                <papertitle>Few-shot Image Generation with Mixup-based Distance Learning</papertitle>
              </a>
              <br>
              <strong>Chaerin Kong</strong>,
              <a href="https://scholar.google.com/citations?user=FbufdJ8AAAAJ&hl=ko">Jeesoo Kim</a>,
              <a href="http://mipal.snu.ac.kr/index.php/Donghoon_Han">Donghoon Han</a>,
              <a href="http://mipal.snu.ac.kr/index.php/Nojun_Kwak">Nojun Kwak</a>
              <br>
              <em>ECCV</em> 2022
              <br>
              <a href="https://github.com/reyllama/mixdl">code</a>
              /
              <a href="https://arxiv.org/abs/2111.11672">arXiv</a>
              /
              <a href="data/MixDL_poster.pdf">poster</a>
              <p></p>
              <p>
              Instead of directly combatting memorization for few-shot (n<100) image synthesis, we propose latent space smoothing regularizations that empower the generator to produce diverse (perceptually continuous) set of samples.
              </p>
            </td>
          </tr>

        </tbody></table>

				
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Under Review</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>

          <tr onmouseout="malle_stop()" onmouseover="malle_start()">
                <td style="padding:20px;width:25%;vertical-align:top">
                  <div class="one">
                    <img src='images/conpro.png' width="160">
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:top">
                  <a href="https://reyllama.github.io/conpro">
                    <papertitle>Conservative Generator, Progressive Discriminator: Coordination of Adversaries in Incremental Few-shot Image Synthesis</papertitle>
                  </a>
                  <br>
                  <strong>Chaerin Kong</strong>,
                  <a href="http://mipal.snu.ac.kr/index.php/Nojun_Kwak">Nojun Kwak</a>
                  <br>
                  <a href="https://arxiv.org/abs/2207.14491">arXiv</a>
                  <p></p>
                  <p>
                  We tackle the challenging task of few-shot incremental image synthesis by training a knowledge-preserving (conservative) generator and semantic learning (progressive) discriminator.
                  </p>
                </td>
              </tr>
					
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                The template is from <a href="https://jonbarron.info">Jon Barron</a>. Thank you for sharing!
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
