
<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>OneR</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- <base href="/"> -->

        <!--FACEBOOK-->
    <meta property="og:image" content="https://jonbarron.info/mipnerf/img/rays_square.png">
    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="682">
    <meta property="og:image:height" content="682">
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://reyllama.github.io/OneR"/>
    <meta property="og:title" content="Fashion-Diffusion" />
    <meta property="og:description" content="Project page for Unifying Vision-Language Representation Space with Single-tower Transformer" />

        <!--TWITTER-->
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="mip-NeRF" />
    <meta name="twitter:description" content="Project page for Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields." />
    <meta name="twitter:image" content="https://jonbarron.info/mipnerf/img/rays_square.png" />


<!--     <link rel="apple-touch-icon" href="apple-touch-icon.png"> -->
  <!-- <link rel="icon" type="image/png" href="img/seal_icon.png"> -->
    <!-- Place favicon.ico in the root directory -->

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <link rel="stylesheet" href="css/bootstrap.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>

    <script src="js/app.js"></script>
</head>

<body>
    <div class="container" id="main">
        <div class="row">
            <h2 class="col-md-12 text-center">
                Unifying Vision-Language Representation Space with <br> Single-tower Transformer</br>
<!--                <small>-->
<!--                    ICCV 2021 (Oral, Best Paper Honorable Mention)-->
<!--                </small>-->
            </h2>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        <a href="https://reyllama.github.io">
                          Chaerin Kong*
                        </a>
                        </br>Seoul National University
                    </li>
                    <li>
                        <a href="https://www.linkedin.com/in/jiho-jang-58580b183/">
                          Jiho Jang*
                        </a>
                        </br>Seoul National University
                    </li><br>
                    <li>
                        <a href="https://scholar.google.com/citations?user=2kW3474AAAAJ&hl=ko">
                            Donghyeon Jeon
                        </a>
                        </br>NAVER
                    </li>
                    <li>
                        <a href="https://www.linkedin.com/in/seonhoon-kim-b9511410b/">
                          Seonhoon Kim
                        </a>
                        </br>NAVER
                    </li>
                    <li>
                        <a href="http://mipal.snu.ac.kr/index.php/Nojun_Kwak">
                          Nojun Kwak
                        </a>
                        </br>Seoul National University
                    </li>
                </ul>
            </div>
        </div>


<!--        <div class="row">-->
<!--                <div class="col-md-4 col-md-offset-4 text-center">-->
<!--                    <ul class="nav nav-pills nav-justified">-->
<!--                        <li>-->
<!--                            <a href="https://arxiv.org/abs/2103.13415">-->
<!--                            <image src="img/mip_paper_image.jpg" height="60px">-->
<!--                                <h4><strong>Paper</strong></h4>-->
<!--                            </a>-->
<!--                        </li>-->
<!--                        <li>-->
<!--                            <a href="https://youtu.be/EpH175PY1A0">-->
<!--                            <image src="img/youtube_icon.png" height="60px">-->
<!--                                <h4><strong>Video</strong></h4>-->
<!--                            </a>-->
<!--                        </li>-->
<!--                        <li>-->
<!--                            <a href="https://github.com/google/mipnerf">-->
<!--                            <image src="img/github.png" height="60px">-->
<!--                                <h4><strong>Code</strong></h4>-->
<!--                            </a>-->
<!--                        </li>-->
<!--                    </ul>-->
<!--                </div>-->
<!--        </div>-->



        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Abstract
                </h3>
                <image src="img/pipeline.jpeg" class="img-responsive" alt="overview"><br>
                <p class="text-justify">
                    Contrastive learning is a form of distance learning that aims to learn invariant features from two related representations. In this paper, we explore the bold hypothesis that
                    an image and its caption can be simply regarded as two different views of the underlying mutual information, and train a model to learn a unified vision-language representation
                    space that encodes both modalities at once in a modality-agnostic manner. We first identify difficulties in learning a generic one-tower model for vision-language pretraining (VLP),
                    and propose OneR as a simple yet effective framework for our goal. We discover intriguing properties that distinguish OneR from the previous works that learn modality-specific representation
                    spaces such as zero-shot object localization, text-guided visual reasoning and multi-modal retrieval, and present analyses to provide insights into this new form of multi-modal representation learning.
                    Thorough evaluations demonstrate the potential of a unified modality-agnostic VLP framework.
                </p>
            </div>
        </div>



<!--        <div class="row">-->
<!--            <div class="col-md-8 col-md-offset-2">-->
<!--                <h3>-->
<!--                    Video-->
<!--                </h3>-->
<!--                <div class="text-center">-->
<!--                    <div style="position:relative;padding-top:56.25%;">-->
<!--                        <iframe src="https://www.youtube.com/embed/EpH175PY1A0" allowfullscreen style="position:absolute;top:0;left:0;width:100%;height:100%;"></iframe>-->
<!--                    </div>-->
<!--                </div>-->
<!--            </div>-->
<!--        </div>-->


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    One-tower Contrastive Learning
                </h3>
                <image src="img/TOWER.png" class="img-responsive" alt="overview">
                <p>
                    Vision-language models can be typically assigned to one of the four architecture prototypes. In this paper, we explore V-L representation learning with <em>single-tower</em> transformer, with a modality-agnostic transformer as well as generic projection layer.
                    However, single-tower models struggle to embed information from two distant modalities (<em>i.e.,</em> image and text) due to the innate modality gap, as shown by the t-SNE plot below.
                </p>
                <image src="img/tsne.png" class="img-responsive" alt="overview">
                <p>
                    We observe that integrating cross-modal mixup contrastive (XMC) objective to the single-tower framework successfully mitigates the modality gap and enables unified representation learning. We take a step further to devise Contextual Invariance Contrastive (CIC) objective,
                    that enforces the final representation formed from either using image or text as the context (<em>K, V in attention layers</em>) to be similar to each other. By combining the two, XMC anc CIC, we stabilize the single-tower unified representation learning and improve
                    the representation quality compared to two-tower baselines (<em>e.g.,</em> CLIP) even with significantly less parameters.
                </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Intriguing Properties of One Representation (OneR)
                </h3>
                <image src="img/OneR_main.png" class="img-responsive" alt="overview"><br>
                <p>
                    As OneR treats individual image patch and text embedding as a generic unit of information and learns to attend between each other indifferently, we discover many interesting properties such as excellent zero-shot object localization, text-guided visual reasoning and multi-modal retrieval.
                    For object localization, we do not rely on any specific algorithms like Grad-CAM, but simply computes patch-text embedding similarities and visualize the highlighted regions. Figure below presents a more vivid comparison with competitive baselines, which employ grad-cam as patch-level similarity operation yields highly unreasonable results.
                </p>
                <image src="img/OneR_compare.png" class="img-responsive" alt="overview">
                <p>
                    More than that, as OneR learns a single representation space for both vision and language modalities, it shows better cross-modal transfer (benefits in one modality performance after additional training in the other modality) than the baseline.
                </p>
                <image src="img/OneR_tab.png" class="img-responsive" alt="overview" width="70%">
                    <p>
                        Above all, OneR records highly comparable results on traditional benchmarks like COCO retrieval even with no initialization prior and significantly less number of parameters.
                    </p>
                    <image src="img/OneR_tab2.png" class="img-responsive" alt="overview">
            </div>
        </div>


<!--        <div class="row">-->
<!--            <div class="col-md-8 col-md-offset-2">-->
<!--                <h3>-->
<!--                    Citation-->
<!--                </h3>-->
<!--                <div class="form-group col-md-10 col-md-offset-1">-->
<!--                    <textarea id="bibtex" class="form-control" readonly>-->
<!--@article{barron2021mipnerf,-->
<!--    title={Mip-NeRF: A Multiscale Representation -->
<!--           for Anti-Aliasing Neural Radiance Fields},-->
<!--    author={Jonathan T. Barron and Ben Mildenhall and -->
<!--            Matthew Tancik and Peter Hedman and -->
<!--            Ricardo Martin-Brualla and Pratul P. Srinivasan},-->
<!--    journal={ICCV},-->
<!--    year={2021}-->
<!--}</textarea>-->
<!--                </div>-->
<!--            </div>-->
<!--        </div>-->

<!--        <div class="row">-->
<!--            <div class="col-md-8 col-md-offset-2">-->
<!--                <h3>-->
<!--                    Acknowledgements-->
<!--                </h3>-->
<!--                <p class="text-justify">-->
<!--                We thank Janne Kontkanen and David Salesin for their comments on the text, Paul Debevec for constructive discussions, and Boyang Deng for JaxNeRF. -->
<!--                    <br>-->
<!--                MT is funded by an NSF Graduate Fellowship.-->
<!--                    <br>-->
<!--                The website template was borrowed from <a href="http://mgharbi.com/">Michaël Gharbi</a>.-->
<!--                </p>-->
<!--            </div>-->
<!--        </div>-->
    </div>
</body>
</html>
